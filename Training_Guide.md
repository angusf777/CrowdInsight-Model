# Kickstarter Project Success Prediction Model Training Guide

This document provides detailed training guidance for the Kickstarter project success prediction model, primarily for technical staff who need to train the model and explain prediction results.

## 1. Model Architecture Overview

The model uses a multimodal neural network architecture that can process multiple types of input features:

- **Text embedding features**: Vector representations generated by pre-trained models
- **Category embedding features**: Such as project category, subcategory, and country
- **Numerical features**: Such as funding goal, project description length, etc.

Model structure:
1. Independent feature processing layers: Process each feature type separately
2. Feature fusion layer: Connect all feature representations together
3. Fully connected layers: Deep learning on fused features
4. Output layer: Predict project success probability
5. SHAP explanation layer: Provide feature importance explanations

### 1.1 Optimal Configuration

Based on empirical evaluation, the following configuration has shown excellent performance:

```python
model = KickstarterModel(
    desc_embedding_dim=768,
    blurb_embedding_dim=384,
    risk_embedding_dim=384,
    subcategory_embedding_dim=100,
    category_embedding_dim=15,
    country_embedding_dim=100,
    numerical_features_dim=9,
    hidden_dim=256,
    dropout_rate=0.3
)
```

This configuration achieves:
- ROC-AUC: 0.961
- Brier Score: 0.075
- Log Loss: 0.248
- Accuracy: 90.24%
- F1 Score: 0.922

## 2. Data Preparation

### 2.1 Data File Format

Training data should be in JSON format, with each record containing all features and labels for a Kickstarter project.

Key field descriptions:
- Embedding vectors must maintain correct dimensions
- Numerical features are recommended to be standardized
- Labels must be binary values (1 for success, 0 for failure)

### 2.2 Data Splitting Strategy

The system defaults to an 80-10-10 train-validation-test split strategy:
- 80% for training the model
- 10% for validation and early stopping
- 10% for final evaluation

Stratified sampling is used during splitting to ensure similar success/failure project ratios in each subset.

## 3. Training Process

### 3.1 Basic Training Flow

1. Data loading and preprocessing
2. Model initialization
3. Training loop
   - Forward propagation
   - Loss calculation (binary cross-entropy)
   - Backward propagation and optimization
   - Validation evaluation
4. Early stopping mechanism
5. Best model saving

### 3.2 Hyperparameter Selection

The most important hyperparameters include:

| Hyperparameter | Suggested Range | Impact | Optimal Value* |
|----------------|-----------------|--------|---------------|
| Learning rate | 0.0001 - 0.001 | Affects convergence speed and stability | 0.001 |
| Batch size | 16 - 64 | Affects training speed and generalization ability | 32 |
| Hidden layer dimensions | 256 - 1024 | Affects model capacity | 256 |
| Dropout rate | 0.2 - 0.4 | Affects regularization strength | 0.3 |

*Based on current best model

Recommendations for different dataset sizes:
- Small datasets (<10000 records): Smaller hidden layer dimensions, higher regularization
- Large datasets (>50000 records): Larger hidden layer dimensions, lower regularization

### 3.3 Train.py Usage Guide

The `train.py` script is the main entry point for training the model. This section provides comprehensive guidance on using this script effectively.

#### Basic Usage

To train the model with default parameters:

```bash
python train.py --data_path data/allProcessed.json
```

#### Available Command-line Arguments

| Argument | Description | Default Value | Example |
|----------|-------------|---------------|---------|
| `--data_path` | Path to the training data file | data/allProcessed.json | `--data_path custom_data.json` |
| `--batch_size` | Number of samples per batch | 16 | `--batch_size 32` |
| `--hidden_dim` | Hidden layer dimension | 256 | `--hidden_dim 512` |
| `--dropout_rate` | Dropout rate for regularization | 0.4 | `--dropout_rate 0.3` |
| `--learning_rate` | Learning rate for optimizer | 0.001 | `--learning_rate 0.0005` |
| `--weight_decay` | Weight decay for L2 regularization | 1e-6 | `--weight_decay 1e-5` |
| `--num_epochs` | Maximum number of training epochs | 50 | `--num_epochs 100` |
| `--early_stop_patience` | Patience for early stopping | 10 | `--early_stop_patience 15` |
| `--optimize_hyperparams` | Enable hyperparameter optimization | False | `--optimize_hyperparams` |
| `--test_size` | Test set proportion | 0.1 | `--test_size 0.15` |
| `--val_size` | Validation set proportion | 0.1 | `--val_size 0.15` |
| `--checkpoint_dir` | Directory to save model checkpoints | models | `--checkpoint_dir custom_models` |
| `--log_dir` | Directory to save training logs | logs | `--log_dir custom_logs` |

#### Training Scenarios

Here are some common training scenarios:

**1. Quick Training with Default Parameters**

```bash
python train.py --data_path data/allProcessed.json
```

**2. Training with Custom Model Architecture**

```bash
python train.py --data_path data/allProcessed.json --hidden_dim 512 --dropout_rate 0.3
```

**3. Training with Different Learning Parameters**

```bash
python train.py --data_path data/allProcessed.json --learning_rate 0.0005 --batch_size 64 --num_epochs 100
```

**4. Hyperparameter Optimization**

```bash
python train.py --data_path data/allProcessed.json --optimize_hyperparams
```

This will automatically search for the best combination of hyperparameters using a grid search approach. The script will try different combinations of:
- Hidden dimensions
- Dropout rates
- Learning rates
- Batch sizes

**5. Custom Train-Val-Test Split**

```bash
python train.py --data_path data/allProcessed.json --test_size 0.15 --val_size 0.15
```

This changes the default 80-10-10 split to a 70-15-15 split.

**6. Continue Training from Checkpoint**

To continue training from a previously saved checkpoint, you can use the default checkpoint directory:

```bash
python train.py --data_path data/allProcessed.json --checkpoint_dir models
```

The script will automatically load the latest checkpoint from the specified directory.

#### Training Outputs

The training process generates the following outputs:

1. **Model Checkpoints**: Saved in the `--checkpoint_dir` directory (default: models/)
   - `best_model.pth`: The best model based on validation AUC
   - `training_params.json`: The parameters used for training

2. **TensorBoard Logs**: Saved in the `--log_dir` directory (default: logs/)
   - Training and validation losses
   - Validation metrics (AUC, Brier score, Log loss)
   - Learning rate changes

3. **Console Output**: Displays real-time training progress
   - Epoch-wise training and validation losses
   - Validation metrics after each epoch
   - Early stopping information

To view the TensorBoard logs, run:

```bash
tensorboard --logdir logs
```

### 3.4 Hyperparameter Optimization

Use the `--optimize_hyperparams` option to enable automatic hyperparameter optimization:

```bash
python train.py --data_path data/allProcessed.json --optimize_hyperparams
```

The system will try multiple hyperparameter combinations and select the configuration with the best validation set performance.

### 3.5 Training Monitoring

During training, pay attention to the following metrics:
- Training loss: Should gradually decrease
- Validation AUC: Should increase and eventually stabilize
- Validation Brier score: Measures the accuracy of probability predictions
- Validation Log Loss: Measures probability calibration

Consider stopping early or adjusting hyperparameters if the following occurs:
- Training loss continues to decrease but validation loss increases (overfitting)
- Validation AUC no longer improves for a long time
- Training process is unstable (large loss fluctuations)

## 4. Evaluating the Model

### 4.1 Evaluation Metrics

Core metrics and targets:

| Metric | Target | Current Best Performance |
|--------|--------|--------------------------|
| ROC-AUC | ≥0.75 | 0.961 |
| Brier Score | ≤0.2 | 0.075 |
| Log Loss | ≤0.6 | 0.248 |
| Accuracy | - | 90.24% |
| F1 Score | - | 0.922 |
| Precision | - | 0.906 |
| Recall | - | 0.938 |

### 4.2 Understanding Evaluation Results

1. **AUC Analysis**:
   - AUC > 0.9: Excellent discrimination ability (current model: 0.961)
   - 0.8 < AUC < 0.9: Good discrimination ability
   - 0.7 < AUC < 0.8: Average discrimination ability
   - AUC < 0.7: Insufficient discrimination ability

2. **Brier Score Interpretation**:
   - Measures the mean squared error between predicted probabilities and actual outcomes
   - Closer to 0 indicates more accurate probability predictions
   - Current model: 0.075 (excellent calibration)

3. **Log Loss Interpretation**:
   - Measures the uncertainty of prediction probabilities
   - Lower Log Loss indicates more confident and accurate predictions
   - Current model: 0.248 (well-calibrated predictions)

### 4.3 Running Evaluation

To run a comprehensive evaluation of the model, use:

```bash
python testing_analysis.py
```

This generates the following outputs in the evaluation/ directory:
- accuracy_results.json: Detailed metrics and SHAP analysis
- roc_curve.png: ROC curve visualization
- feature_importance.png: Feature importance visualization

### 4.4 Model Comparison

When training multiple model versions, it's recommended to build a comparison table:

| Model Version | Hidden Dim | Dropout | Learning Rate | Validation AUC | Test AUC | Brier | Log Loss |
|---------------|------------|---------|--------------|----------------|----------|-------|----------|
| v1 | 512 | 0.3 | 0.001 | 0.94 | 0.93 | 0.09 | 0.26 |
| v2 (current) | 256 | 0.3 | 0.001 | 0.959 | 0.961 | 0.075 | 0.248 |

## 5. Explaining Prediction Results

### 5.1 SHAP Value Interpretation

SHAP values reflect each feature's contribution to the prediction:
- **Positive SHAP values**: Features push prediction toward "success"
- **Negative SHAP values**: Features push prediction toward "failure"
- **SHAP value absolute value**: Strength of feature influence

### 5.2 Feature Importance Analysis

Based on SHAP values from the evaluation, the most important factors for success prediction are:

| Feature | Importance | Mean SHAP | Impact Direction |
|---------|------------|-----------|------------------|
| funding_goal | 0.657 | -0.657 | Negative |
| description_embedding | 0.119 | -0.119 | Negative |
| description_length | 0.006 | -0.003 | Mixed |
| risk_embedding | 0.006 | -0.0003 | Mixed |
| subcategory_embedding | 0.006 | 0.002 | Positive |

**Typical Success Factors**:
- High-quality project description (positive description_embedding values)
- Reasonable funding goal (lower to moderate goals)
- Appropriate campaign duration (typically around 30 days)
- Sufficient number of images and videos
- Good historical success rate

**Typical Failure Factors**:
- Excessively high funding goal (strongest negative factor)
- Poor description quality
- Lack of image and video content
- Too short or too long campaign duration
- Creator has no historically successful projects

### 5.3 Case Analysis

Here is an example case analysis:

```
Project A: Predicted success probability of 0.85

Main contributing factors:
[+0.20] Description length: Very detailed project description increases success probability
[+0.15] Historical success rate: Creator has a good track record
[-0.05] Funding goal: Relatively high goal slightly decreases success probability
```

Interpretation: This project is likely to succeed, mainly due to detailed project description and the creator's successful history, despite a slightly high funding goal. 